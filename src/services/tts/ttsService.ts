import { useCallback, useEffect, useMemo, useRef } from 'react'


import { cleanText } from './textUtils'
import { approximateWhisperDataFromText } from './whisperDataGenerator'
import { type TTSConfig, type WhisperData, defaultTTSConfig } from './ttsConfig'

// Workerized TTS using vits-web library
const createTTSWorker = (): Worker => {
  const ttsWorkerString = `
import * as tts from 'https://cdn.jsdelivr.net/npm/@diffusionstudio/vits-web@1.0.3/+esm'
self.addEventListener('message', async (e) => {
  const id = e.data.id
  const text = e.data.text || 'please supply some text'
  const voiceId = e.data.voice || 'en_GB-hfc_male-medium'
  logger.log('TTS Worker received message:', { id, text, voiceId })
  tts.predict({text, voiceId}).then(audio => {
    new Promise((resolve, reject) => {
      const reader = new FileReader()
      reader.onload = () => resolve(reader.result)
      reader.onerror = () => reject(reader.error)
      reader.readAsArrayBuffer(audio)
    }).then(audio => {
      self.postMessage({ audio, id })
    }).catch(error => {
      logger.error('TTS Worker Error:', error)
      self.postMessage({ error: error.message, id })
    })
  })
})
`
  return new Worker(URL.createObjectURL(new Blob([ttsWorkerString], { type: 'text/javascript' })), { type: 'module' })
}

interface TTSServiceOptions {
  config?: Partial<TTSConfig>
  onSpeechStart?: (text: string, whisperData: WhisperData) => void
  onSpeechEnd?: () => void
}

interface QueuedMessage {
  id: number
  text: string
  status: 'pending' | 'generating' | 'ready' | 'playing' | 'completed' | 'error'
  audio?: ArrayBuffer
  whisperData?: WhisperData
  error?: string
}

let globalId = 0

export const useTTSService = (options: TTSServiceOptions = {}) => {
  const { config: configPartial = {}, onSpeechStart, onSpeechEnd } = options
  const config = useMemo(() => ({ ...defaultTTSConfig, ...configPartial }), [configPartial])
  const isProcessing = useRef(false)
  const messageQueue = useRef<QueuedMessage[]>([])
  const nextExpectedId = useRef(0)
  const workerRef = useRef<Worker | null>(null)
  const currentResolveRef = useRef<(() => void) | null>(null)

  // Initialize worker for local TTS
  const getOrCreateWorker = useCallback(() => {
    if (!workerRef.current) {
      workerRef.current = createTTSWorker()
    }
    return workerRef.current
  }, [])

  const playAudioBuffer = useCallback(async (audioBuffer: ArrayBuffer): Promise<void> => {
    return new Promise((resolve, reject) => {
      const audioContext = new AudioContext()

      audioContext
        .decodeAudioData(audioBuffer.slice(0))
        .then((decodedAudio) => {
          const source = audioContext.createBufferSource()
          const gainNode = audioContext.createGain()
          
          // Use global volume if available, otherwise use config volume
          const volume = (window as Window & { globalTTSVolume?: number }).globalTTSVolume ?? config.volume
          gainNode.gain.value = volume
          
          source.buffer = decodedAudio
          source.connect(gainNode)
          gainNode.connect(audioContext.destination)

          source.onended = () => {
            resolve()
          }

          source.start(0)
        })
        .catch((error) => {
          logger.error('Audio decode error:', error)
          reject(error)
        })
    })
  }, [config.volume])

  // Generate audio using browser speech synthesis
  const generateBrowserAudio = useCallback(async (_message: QueuedMessage) => {
    if (!('speechSynthesis' in window)) {
      throw new Error('Speech synthesis not supported')
    }

    // For browser mode, we don't pre-generate audio, we play it directly during playback
    // Just mark as ready
    return Promise.resolve()
  }, [])

  // Generate audio using local worker
  const generateLocalAudio = useCallback(
    async (message: QueuedMessage) => {
      return new Promise<void>((resolve, reject) => {
        const worker = getOrCreateWorker()

        const handleWorkerMessage = (event: MessageEvent) => {
          const { audio, id, error } = event.data
          if (error) {
            reject(new Error(error))
            return
          }
          if (id === message.id) {
            message.audio = audio
            worker.removeEventListener('message', handleWorkerMessage)
            worker.removeEventListener('error', handleWorkerError)
            resolve()
          }
        }

        const handleWorkerError = (error: ErrorEvent) => {
          worker.removeEventListener('message', handleWorkerMessage)
          worker.removeEventListener('error', handleWorkerError)
          reject(new Error(`Worker Error: ${error.message}`))
        }

        worker.addEventListener('message', handleWorkerMessage)
        worker.addEventListener('error', handleWorkerError)

        worker.postMessage({
          id: message.id,
          text: message.text,
          voice: config.voice
        })
      })
    },
    [getOrCreateWorker, config.voice]
  )

  // Generate audio using remote API
  const generateRemoteAudio = useCallback(
    async (message: QueuedMessage) => {
      if (!config.apiKey) {
        throw new Error('API key required for remote TTS')
      }

      const url = config.url || 'https://api.openai.com/v1/audio/speech'

      const headers: Record<string, string> = {
        'Content-Type': 'application/json',
        Authorization: `Bearer ${config.apiKey}`
      }

      // OpenAI TTS API payload format
      const requestBody = {
        input: message.text,
        model: config.model || 'tts-1',
        voice: config.voice || 'shimmer',
        response_format: 'mp3',
        speed: config.speed || 1.0
      }

      const response = await fetch(url, {
        method: 'POST',
        headers,
        body: JSON.stringify(requestBody)
      })

      if (!response.ok) {
        throw new Error(`Remote TTS API error: ${response.status} ${response.statusText}`)
      }

      message.audio = await response.arrayBuffer()
    },
    [config.apiKey, config.url, config.model, config.voice, config.speed]
  )

  // Generate audio for a specific message
  const generateAudioForMessage = useCallback(
    async (message: QueuedMessage) => {
      message.status = 'generating'
      logger.log(`Generating audio for message ${message.id}`)

      try {
        if (config.mode === 'remote') {
          await generateRemoteAudio(message)
        } else if (config.mode === 'browser') {
          await generateBrowserAudio(message)
        } else if (config.mode === 'local') {
          await generateLocalAudio(message)
        }
        message.status = 'ready'
      } catch (error) {
        logger.error(`Error generating audio for message ${message.id}:`, error)
        message.status = 'error'
        message.error = error instanceof Error ? error.message : 'Unknown error'
      }
    },
    [config.mode, generateRemoteAudio, generateBrowserAudio, generateLocalAudio]
  )

  // Process the queue sequentially
  const processQueue = useCallback(async () => {
    if (isProcessing.current) return
    isProcessing.current = true

    try {
      while (true) {
        const nextMessage = messageQueue.current.find((msg) => msg.id === nextExpectedId.current)

        if (!nextMessage) {
          // No more messages to process
          break
        }

        // If message is pending, generate audio
        if (nextMessage.status === 'pending') {
          await generateAudioForMessage(nextMessage)
        }

        // If message is not ready after generation, skip for now
        if (nextMessage.status !== 'ready') {
          if (nextMessage.status === 'error') {
            logger.error(`Skipping message ${nextMessage.id} due to error:`, nextMessage.error)
            messageQueue.current = messageQueue.current.filter((msg) => msg.id !== nextMessage.id)
            nextExpectedId.current = nextMessage.id + 1
            continue
          } else if (nextMessage.status === 'generating') {
            // For local mode, wait a bit for generation to complete
            if (config.mode === 'local') {
              await new Promise<void>((resolve) => requestAnimationFrame(() => resolve()))
              continue
            }
          }
          // For other cases, exit and let it be handled later
          break
        }

        // Play the message
        nextMessage.status = 'playing'
        logger.log(`Playing message ${nextMessage.id}`)

        // Generate whisper data and notify speech start
        const whisperData = approximateWhisperDataFromText(nextMessage.text, config.speed, 0)
        if (onSpeechStart) {
          onSpeechStart(nextMessage.text, whisperData)
        }

        try {
          // Handle different TTS modes for playback
          if (config.mode === 'browser') {
            await new Promise<void>((resolve, reject) => {
              // Store resolve function so we can call it when stopping
              currentResolveRef.current = resolve
              
              const utterance = new SpeechSynthesisUtterance(nextMessage.text)
              utterance.rate = config.speed
              utterance.pitch = config.pitch
              // Use global volume if available, otherwise fall back to config volume
              utterance.volume = (window as Window & { globalTTSVolume?: number }).globalTTSVolume ?? config.volume

              const selectBrowserVoice = (voices: SpeechSynthesisVoice[]) => {
                const desired = (config.voice || '').trim().toLowerCase()
                if (!desired) return

                // If user provided an actual browser voice name, prefer exact match.
                const exact = voices.find((v) => v.name.toLowerCase() === desired)
                if (exact) {
                  utterance.voice = exact
                  logger.log('TTS: Using exact browser voice match:', exact.name, exact.lang)
                  return
                }

                // Otherwise, fall back to gender/language heuristics based on config.voice.
                const isMale = desired.includes('male')
                const isFemale = desired.includes('female')
                if (!isMale && !isFemale) return

                const findVoiceByLang = (lang: string, male: boolean) =>
                  voices.find((voice) => {
                    if (voice.lang !== lang) return false
                    const name = voice.name.toLowerCase()
                    if (male) {
                      return (
                        name.includes('male') ||
                        name.includes('rishi') ||
                        (!name.includes('female') && !name.includes('veena') && !name.includes('alice'))
                      )
                    }
                    return name.includes('female') || name.includes('veena') || name.includes('alice')
                  })

                if (isMale) {
                  // Priority: Indian English -> UK English -> US English -> Default
                  const maleVoice =
                    findVoiceByLang('en-IN', true) ||
                    findVoiceByLang('en-GB', true) ||
                    findVoiceByLang('en-US', true) ||
                    voices.find((v) => v.name.toLowerCase().includes('male'))

                  if (maleVoice) {
                    utterance.voice = maleVoice
                    logger.log('TTS: Using male voice:', maleVoice.name, maleVoice.lang)
                  }
                } else if (isFemale) {
                  const femaleVoice =
                    findVoiceByLang('en-IN', false) ||
                    findVoiceByLang('en-GB', false) ||
                    findVoiceByLang('en-US', false) ||
                    voices.find((v) => v.name.toLowerCase().includes('female'))

                  if (femaleVoice) {
                    utterance.voice = femaleVoice
                    logger.log('TTS: Using female voice:', femaleVoice.name, femaleVoice.lang)
                  }
                }
              }
              
              utterance.onend = () => {
                currentResolveRef.current = null
                resolve()
              }
              
              utterance.onerror = (event) => {
                currentResolveRef.current = null
                reject(new Error(`Browser TTS Error: ${event.error}`))
              }

              // Many browsers populate voices asynchronously. If not ready, wait for `voiceschanged`
              // (no timeouts) and fall back to speaking on the next frame.
              let didSpeak = false
              const onVoicesChanged = () => {
                if (didSpeak) return
                didSpeak = true
                speechSynthesis.removeEventListener('voiceschanged', onVoicesChanged)
                const voices = speechSynthesis.getVoices()
                if (voices.length > 0) selectBrowserVoice(voices)
                speechSynthesis.speak(utterance)
              }

              const initialVoices = speechSynthesis.getVoices()
              if (initialVoices.length > 0) {
                didSpeak = true
                selectBrowserVoice(initialVoices)
                speechSynthesis.speak(utterance)
              } else {
                speechSynthesis.addEventListener('voiceschanged', onVoicesChanged)
                speechSynthesis.getVoices() // kick voice loading in some browsers
                requestAnimationFrame(() => {
                  if (didSpeak) return
                  const voices = speechSynthesis.getVoices()
                  if (voices.length > 0) {
                    onVoicesChanged()
                  } else {
                    // Fall back to default voice if voices never populate.
                    didSpeak = true
                    speechSynthesis.removeEventListener('voiceschanged', onVoicesChanged)
                    speechSynthesis.speak(utterance)
                  }
                })
              }
            })
          } else if (nextMessage.audio && nextMessage.audio.byteLength > 0) {
            await playAudioBuffer(nextMessage.audio)
          }
        } catch (playbackError) {
          logger.error(`Error playing message ${nextMessage.id}:`, playbackError)
        }

        // Mark as completed and remove from queue
        nextMessage.status = 'completed'
        messageQueue.current = messageQueue.current.filter((msg) => msg.id !== nextMessage.id)
        nextExpectedId.current = Math.max(nextExpectedId.current, nextMessage.id + 1) // if we have skipped ahead by stopping speech, we need to set the next expected id to the next message id
      }

      // Call onSpeechEnd only when queue is empty
      if (onSpeechEnd && messageQueue.current.length === 0) {
        onSpeechEnd()
      }
    } catch (error) {
      logger.error('Queue processing error:', error)
      if (onSpeechEnd) {
        onSpeechEnd()
      }
    } finally {
      isProcessing.current = false
    }
  }, [onSpeechEnd, config.speed, config.mode, config.pitch, config.volume, config.voice, onSpeechStart, generateAudioForMessage, playAudioBuffer])

  // Add message to queue and assign ID
  const speak = useCallback(
    async (text: string) => {

      console.log('DEBUG:speak', text)
      if (!text.trim()) return null

      const messageId = globalId++
      const cleanedText = cleanText(text)

      if (!cleanedText.trim()) return messageId

      const queuedMessage: QueuedMessage = {
        id: messageId,
        text: cleanedText,
        status: 'pending'
      }

      messageQueue.current.push(queuedMessage)
      logger.log(`Queued message ${messageId}:`, cleanedText.substring(0, 50))

      // For local mode, start generating all pending messages immediately
      if (config.mode === 'local') {
        const allPendingMessages = messageQueue.current.filter((msg) => msg.status === 'pending')
        allPendingMessages.forEach((message) => {
          generateAudioForMessage(message) // Don't await - let them generate in parallel
        })
      }

      // Start processing if not already running
      if (!isProcessing.current) {
        processQueue()
      }

      return messageId
    },
    [processQueue, config.mode, generateAudioForMessage]
  )

  const stopSpeaking = useCallback(() => {
    // Stop browser speech synthesis
    if ('speechSynthesis' in window) {
      speechSynthesis.cancel()
    }

    // Resolve the current Promise to unblock processQueue
    if (currentResolveRef.current) {
      currentResolveRef.current()
      currentResolveRef.current = null
    }

    // Terminate worker if it exists
    if (workerRef.current) {
      workerRef.current.terminate()
      workerRef.current = null
    }


    const lastMessageId = messageQueue.current.length > 0 ? messageQueue.current[messageQueue.current.length - 1].id + 1 : globalId - 1
    // Clear the queue
    messageQueue.current = []
    nextExpectedId.current = lastMessageId
    isProcessing.current = false

    // Call speech end callback when speech is stopped
    if (onSpeechEnd) {
      onSpeechEnd()
    }
  }, [onSpeechEnd])

  // Cleanup worker on unmount
  useEffect(() => {
    return () => {
      if (workerRef.current) {
        workerRef.current.terminate()
      }
    }
  }, [])

  const getQueueStatus = useCallback(() => {
    return {
      queueLength: messageQueue.current.length,
      isProcessing: isProcessing.current,
      nextExpectedId: nextExpectedId.current,
      messages: messageQueue.current.map((msg) => ({
        id: msg.id,
        status: msg.status,
        text: msg.text.substring(0, 50) + (msg.text.length > 50 ? '...' : '')
      }))
    }
  }, [])


  const state = useMemo(() => ({
    isSpeaking: isProcessing.current,
    audioQueue: messageQueue.current.length
  // eslint-disable-next-line react-hooks/exhaustive-deps
  }), [isProcessing.current, messageQueue.current.length])

  const actions = useMemo(
    () => ({
      speak,
      stopSpeaking,
      playAudioBuffer,
      getQueueStatus
    }),
    [speak, stopSpeaking, playAudioBuffer, getQueueStatus]
  )

  return useMemo(
    () => ({
      state,
      actions
    }),
    [state, actions]
  )
}
